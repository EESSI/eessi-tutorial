{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Practices for CernVM-FS in HPC","text":"<p>This is an introductory tutorial to EESSI, the European Environment for Scientific Software Installations, with a focus on employing it in the context of High-Performance Computing (HPC).</p> <p>In this tutorial you will learn what EESSI is, how to get access to EESSI, how to customise EESSI, and how to use EESSI repositories on HPC infrastructure.</p> <p>Ready to go? Click here to start the tutorial!</p>"},{"location":"#recording","title":"Recording","text":"<p>Once we have a recording of this tutorial available it will appear here.</p>"},{"location":"#slides","title":"Slides","text":"<p>Once we have slides for this tutorial available they will appear here.</p>"},{"location":"#intended-audience","title":"Intended audience","text":"<p>This tutorial is intended for a general audience who are familiar with running software from the command line; no specific prior knowledge or experience is required.</p> <p>We expect it to be most valuable to people who are interested in running scientific software on variety of compute infrastructures.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of Linux shell environment</li> <li>Basic knowledge of Linux file systems</li> <li>Familiarity with High-Performance Computing environments is a plus</li> <li>Hands-on experience with running scientific software workloads is a plus</li> </ul>"},{"location":"#practical-information","title":"Practical information","text":""},{"location":"#slack-channel","title":"Slack channel","text":"<p>Dedicated channel in EESSI Slack: <code>#eessi-tutorial</code></p> <p>Click here to join the EESSI Slack</p>"},{"location":"#multixscale","title":"MultiXscale","text":"<p>This tutorial was developed and organised in the context of the MultiXscale EuroHPC Centre-of-Excellence.</p> <p>Funded by the European Union. This work has received funding from the European High Performance Computing Joint Undertaking (JU) and countries participating in the project under grant agreement No 101093169.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Bob Dr\u00f6ge (University of Groningen, The Netherlands)</li> <li>Kenneth Hoste (Ghent University, Belgium)</li> <li>Alan O'Cais (University of Barcelona, Spain; CECAM)</li> <li>Lara Peeters (Ghent University, Belgium)</li> <li>Thomas R\u00f6blitz (University of Bergen, Norway)</li> <li>Caspar van Leeuwen (SURF, The Netherlands)</li> </ul>"},{"location":"#additional-resources","title":"Additional resources","text":"<ul> <li>TBD</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting for EESSI","text":"<p>Note</p> <p>In this section, we will continue to use the EESSI CernVM-FS repository <code>software.eessi.io</code> as a running example, but the troubleshooting guidelines are by no means specific to EESSI.</p> <p>Make sure you adjust the example commands to the CernVM-FS repository you are using, if needed.</p>"},{"location":"troubleshooting/#typical-problems","title":"Typical problems","text":""},{"location":"troubleshooting/#error-messages","title":"Error messages","text":"<p>The error messages that you may encounter when accessing a CernVM-FS repository are often quite cryptic, especially if you are not very familiar with CernVM-FS, or with file systems and networking on Linux systems in general.</p> <p>Here are a couple of examples:</p> <ul> <li> <p>The CernVM-FS repository may not be known (yet) on your system, which will result   in a (clear) error message like this when you try to access it:   <pre><code>$ ls /cvmfs/software.eessi.io\nls: cannot access '/cvmfs/software.eessi.io': No such file or directory\n</code></pre></p> </li> <li> <p>You may see errors messages that suggest network connectivity problems, like:   <pre><code>Failed to discover HTTP proxy servers (23 - proxy auto-discovery failed)\n</code></pre></p> </li> <li> <p>Other problems may be quite specific to the internals of CernVM-FS,   rather than being configuration or networking issues. Examples include:   <pre><code>Failed to initialize root file catalog (16 - file catalog failure)\n</code></pre> <pre><code>Failed to transfer ownership of /var/lib/cvmfs/shared to cvmfs\n</code></pre> <pre><code>ls: cannot open directory '/cvmfs/config-repo.cern.ch': Too many levels of symbolic links\n</code></pre> <pre><code>Transport endpoint is not connected\n</code></pre>   The last error message indicates that FUSE has failed.</p> </li> </ul> <p>We will give some advice below on how you might figure out what is wrong when seeing error messages like this.</p>"},{"location":"troubleshooting/#general-approach","title":"General approach","text":"<p>In general, it is recommended to take a step-by-step approach to troubleshooting:</p> <ul> <li>WIP;</li> </ul>"},{"location":"troubleshooting/#common-problems","title":"Common problems","text":""},{"location":"troubleshooting/#installation","title":"CernVM-FS installation","text":"<p>Make sure that CernVM-FS is actually installed (correctly).</p> <p>Check whether both the <code>/cvmfs</code> directory and the <code>cvmfs</code> service account exists on the system: <pre><code>ls /cvmfs\nid cvmfs\n</code></pre></p> <p>Either of these errors would be a clear indication that CernVM-FS is not installed, or that the installation was not completed: <pre><code>ls: cannot access '/cvmfs': No such file or directory\n</code></pre> <pre><code>id: \u2018cvmfs\u2019: no such user\n</code></pre></p> <p>You can also check whether the <code>cvmfs2</code> command is available, and working:</p> <pre><code>cvmfs2 --help\n</code></pre> <p>which should produce output that starts with:</p> <pre><code>The CernVM File System\nVersion 2.11.2\n</code></pre>"},{"location":"troubleshooting/#configuration","title":"CernVM-FS configuration","text":"<p>A common issue is incorrectly configuring CernVM-FS, either by making a silly mistake in a configuration file.</p>"},{"location":"troubleshooting/#reloading","title":"Reloading","text":"<p>Don't forget to reload the CernVM-FS configuration after you've made changes to it:</p> <pre><code>sudo cvmfs_config reload\n</code></pre> <p>Note that changes to specific configuration settings, in particular those related to FUSE, will not be reloaded with this command, since they require remounting the repository.</p>"},{"location":"troubleshooting/#show-configuration","title":"Show configuration","text":"<p>Verify the configuration via <code>cvmfs_config showconfig</code>:</p> <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre> <p>Using the <code>-s</code> option, you can trim the output to only show non-empty configuration settings: <pre><code>cvmfs_config showconfig -s software.eessi.io\n</code></pre></p> <p>We strongly advise combining this command with <code>grep</code> to check for specific configuration settings, like:</p> <pre><code>$ cvmfs_config showconfig software.eessi.io | grep CVMFS_SERVER_URL\nCVMFS_SERVER_URL='http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io;http://azure-us-east-s1.eessi.science/cvmfs/software.eessi.io'    # from /cvmfs/cvmfs-config.cern.ch/etc/cvmfs/domain.d/eessi.io.conf\n</code></pre> <p>Be aware that <code>cvmfs_config showconfig</code> will read the configuration files as they are currently, but that does not necessarily mean that those configuration settings are currently active.</p>"},{"location":"troubleshooting/#non-existing-repositories","title":"Non-existing repositories","text":"<p>Keep in mind that <code>cvmfs_config</code> does not check whether the specified repository is actually known at all. Try for example querying the configuration for the fictional <code>vim.or.emacs.io</code> repository:</p> <pre><code>cvmfs_config showconfig vim.or.emacs.io\n</code></pre>"},{"location":"troubleshooting/#active_configuration","title":"Inspect active configuration","text":"<p>Inspect the active configuration that is currently used by talking to the running CernVM-FS service via <code>cvmfs_talk</code>.</p> <p>Note</p> <p>This requires that the specified CernVM-FS repository is currently mounted.</p> <pre><code>ls /cvmfs/software.eessi.io &gt; /dev/null  # to trigger mount if not mounted yet\nsudo cvmfs_talk -i software.eessi.io parameters\n</code></pre> <p><code>cvmfs_talk</code> can also be used to query other live aspects of a particular repository, see the output of <code>cvmfs_talk --help</code>. For example:</p> <ul> <li>The current revision of repository contents (via <code>revision</code>);</li> <li>Information on the Stratum 1 replica server being used (via <code>host ...</code>);</li> <li>Information on the proxy server being used (via <code>proxy ...</code>);</li> <li>Information on the CernVM-FS client cache (via <code>cache ...</code>);</li> </ul>"},{"location":"troubleshooting/#non-mounted-repositories","title":"Non-mounted repositories","text":"<p>If running <code>cvmfs_talk</code> fails with an error like \"<code>Seems like CernVM-FS is not running</code>\", try triggering a mount of the repository first by accessing it (with <code>ls</code>), or by running:</p> <pre><code>cvmfs_config probe software.eessi.io\n</code></pre> <p>If the latter succeeds but accessing the repository does not, there may be an issue with the (active) configuration, or there may be a connectivity problem.</p>"},{"location":"troubleshooting/#repository-public-key","title":"Repository public key","text":"<p>In order for CernVM-FS to access a repository the corresponding public key must be available, in a domain-specific subdirectory of <code>/etc/cvmfs/keys</code>, like: <pre><code>$ ls /etc/cvmfs/keys/cern.ch\ncern-it1.cern.ch.pub  cern-it4.cern.ch.pub  cern-it5.cern.ch.pub\n</code></pre></p> <p>or in the active CernVM-FS config repository, like for EESSI:</p> <pre><code>$ ls /cvmfs/cvmfs-config.cern.ch/etc/cvmfs/keys/eessi.io\neessi.io.pub\n</code></pre>"},{"location":"troubleshooting/#connectivity","title":"Connectivity issues","text":"<p>There could be various issues related to network connectivity, for example a firewall blocking connections.</p> <p>CernVM-FS uses plain <code>HTTP</code> as data transfer protocol, so basic tools can be used to investigate connectivity issues.</p> <p>You should make sure that the client system can connect to the Squid proxy and/or Stratum-1 replica server(s) via the required ports.</p>"},{"location":"troubleshooting/#determine_proxy","title":"Determine proxy server","text":"<p>First figure out if a proxy server is being used via: <pre><code>sudo cvmfs_talk -i software.eessi.io proxy info\n</code></pre></p> <p>This should produce output that looks like:</p> <pre><code>Load-balance groups:\n[0] http://PROXY_IP:3128 (PROXY_IP, +6h)\n[1] DIRECT\nActive proxy: [0] http://PROXY_IP:3128\n</code></pre> <p>(to protect the innocent, the actual proxy IP was replaced with \"<code>PROXY_IP</code>\" in the output above)</p> <p>The last line indicates that a proxy server is indeed being used currently.</p> <p><code>DIRECT</code> would mean that no proxy server is being used.</p>"},{"location":"troubleshooting/#access-to-proxy-server","title":"Access to proxy server","text":"<p>If a proxy server is used, you should check whether it can be accessed at port <code>3128</code> (default Squid port).</p> <p>For this, you can use standard networking tools (if available):</p> <ul> <li><code>nc</code>, ncat, a reimplementation of netcat:   <pre><code>nc -vz PROXY_IP 3128\n</code></pre></li> <li><code>telnet</code>:   <pre><code>telnet PROXY_IP 3128\n</code></pre></li> <li><code>tcptraceroute</code>:   <pre><code>sudo tcptraceroute PROXY_IP 3128\n</code></pre></li> </ul> <p>You will need to replace \"<code>PROXY_IP</code>\" in the commands above with the actual IP (or hostname) of the proxy server being used.</p>"},{"location":"troubleshooting/#determine-stratum-1","title":"Determine Stratum 1","text":"<p>Check which Stratum 1 servers are currently configured:</p> <pre><code>cvmfs_config showconfig software.eessi.io | grep CVMFS_SERVER_URL\n</code></pre> <p>Determine which Stratum 1 is currently being used by CernVM-FS:</p> <pre><code>$ sudo cvmfs_talk -i software.eessi.io host info\n  [0] http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io (unprobed)\n  [1] http://azure-us-east-s1.eessi.science/cvmfs/software.eessi.io (unprobed)\nActive host 0: http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io\n</code></pre> <p>In this case, the public Stratum 1 for EESSI in AWS <code>eu-central</code> is being used: <code>aws-eu-central-s1.eessi.science</code>.</p>"},{"location":"troubleshooting/#access-to-stratum-1","title":"Access to Stratum 1","text":"<p>If no proxy is being used (<code>CVMFS_HTTP_PROXY</code> is set to <code>DIRECT</code>, see also above), you should check whether the active Stratum 1 is directly accessible at port <code>80</code>.</p> <p>Again, you can use standard networking tools for this:</p> <p><pre><code>nc -vz aws-eu-central-s1.eessi.science 80\n</code></pre> <pre><code>telnet aws-eu-central-s1.eessi.science 80\n</code></pre> <pre><code>sudo tcptraceroute aws-eu-central-s1.eessi.science 80\n</code></pre></p>"},{"location":"troubleshooting/#download-from-stratum-1","title":"Download from Stratum 1","text":"<p>To see whether a Stratum 1 replica server can be used to download repository contents from, you can use <code>curl</code> to check whether the <code>.cvmfspublished</code> file is accessible ( this file must exist in every repository ):</p> <pre><code>S1_URL=\"http://aws-eu-central-s1.eessi.science\"\ncurl --head ${S1_URL}/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> <p>If CernVM-FS is configured to use a proxy server, you should let <code>curl</code> use it too: <pre><code>P_URL=\"http://PROXY_IP:3128\"\nS1_URL=\"http://aws-eu-central-s1.eessi.science\"\ncurl --proxy ${P_URL} --head ${S1_URL}/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> or equivalently via the standard <code>http_proxy</code> environment variable that <code>curl</code> picks up on: <pre><code>S1_URL=\"http://aws-eu-central-s1.eessi.science\"\nhttp_proxy=\"PROXY_IP:3128\" curl --head ${S1_URL}/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre></p> <p>Make sure you replace \"<code>PROXY_IP</code>\" in the commands above with the actual IP (or hostname) of the proxy server.</p> <p>If you see a <code>200</code> HTTP return code in the first line of output produced by <code>curl</code>, access is working as it should:</p> <pre><code>HTTP/1.1 200 OK\n</code></pre> <p>If you see <code>403</code> as return code, then something is blocking the connection:</p> <pre><code>HTTP/1.1 403 Forbidden\n</code></pre> <p>In this case, you should check whether a firewall is being used, or whether an ACL in the Squid proxy configuration is the culprit.</p> <p>If you see <code>404</code> as return code, you made a typo in the <code>curl</code> command : <pre><code>HTTP/1.1 404 Not Found\n</code></pre> Maybe you forgot the '<code>.</code>' in <code>.cvmfspublished</code>?</p> <p>Note</p> <p>A Stratum 1 server does not provide access to all possible CernVM-FS repositories.</p>"},{"location":"troubleshooting/#network-latency-bandwidth","title":"Network latency &amp; bandwidth","text":"<p>To check the network latency and bandwidth, you can use <code>iperf3</code> and <code>tcptraceroute</code>.</p>"},{"location":"troubleshooting/#mounting","title":"Mounting problems","text":""},{"location":"troubleshooting/#autofs","title":"<code>autofs</code>","text":"<p>Keep in mind that (by default) CernVM-FS repositories are mounted via <code>autofs</code>.</p> <p>Hence, you should not rely on the output of <code>ls /cvmfs</code> to determine which repositories can be accessed with your current configuration, since they may not be mounted currently.</p> <p>You can check whether a specific repository is available by trying to access it directly:</p> <pre><code>ls /cvmfs/software.eessi.io\n</code></pre>"},{"location":"troubleshooting/#currently-mounted-repositories","title":"Currently mounted repositories","text":"<p>To check which CernVM-FS repositories are currently mounted, run: <pre><code>cvmfs_config stat\n</code></pre></p>"},{"location":"troubleshooting/#probing","title":"Probing","text":"<p>To check whether a repository can be mounted, you can try to probe it:</p> <pre><code>$ cvmfs_config probe software.eessi.io\nProbing /cvmfs/software.eessi.io... OK\n</code></pre>"},{"location":"troubleshooting/#manual-mounting","title":"Manual mounting","text":"<p>If you can not get access to a repository via auto-mounting by <code>autofs</code>, you can try to manually mount it, since that may reveal specific error messages:</p> <pre><code>mkdir -p /tmp/cvmfs/eessi\nsudo mount -t cvmfs software.eessi.io /tmp/cvmfs/eessi\n</code></pre> <p>You can even try using the <code>cvmfs2</code> command directly to mount a repository: <pre><code>mkdir -p /tmp/cvmfs/eessi\nsudo /usr/bin/cvmfs2 -d -f \\\n    -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=$(id -u cvmfs),gid=$(id -g cvmfs),libfuse=3 \\\n    software.eessi.io /tmp/cvmfs/eessi\n</code></pre> which prints lots of information for debugging (option <code>-d</code>).</p>"},{"location":"troubleshooting/#resources","title":"Insufficient resources","text":"<p>Keep in mind that the problems you observe may be the result of a shortage in resources, for example:</p> <ul> <li>Lack of sufficient memory, for example for the kernel file system cache, which will typically   lead to degrated (start-up) performance;</li> <li>Lack of sufficient disk space, for the CernVM-FS client cache, for the proxy server,   or for the private Stratum 1 replica server;</li> <li>Network latency issues, either within the local network (to the proxy server or Stratum 1 replica server),   or to the outside world (public Stratum 1 replica servers) \u2013 see also the Connectivity   section;</li> </ul>"},{"location":"troubleshooting/#caching","title":"Caching woes","text":"<p>CernVM-FS assumes that the local cache directory is trustworthy.</p> <p>Although unlikely, problems you are observing could be caused by some form of corruption in the CernVM-FS client cache, for example due to problems outside of the control of CernVM-FS (like a disk partition running full).</p> <p>Even in the absence of problems it may still be interesting to inspect the contents of the client cache, for example when trying to understand performance-related problems.</p>"},{"location":"troubleshooting/#checking-cache-usage","title":"Checking cache usage","text":"<p>To check the current usage of the client cache across all repositories, you can use:</p> <pre><code>cvmfs_config stat -v\n</code></pre> <p>You can get machine-readable output by not using the <code>-v</code> option (which is for getting human-readable output).</p> <p>To only get information on cache usage for a particular repository, pass it as an extra argument:</p> <pre><code>cvmfs_config stat -v software.eessi.io\n</code></pre> <p>To check overall cache size, use <code>du</code> on the cache directory (determined by <code>CVMFS_CACHE_BASE</code>):</p> <pre><code>$ sudo du -sh /var/lib/cvmfs\n1.1G    /var/lib/cvmfs\n</code></pre>"},{"location":"troubleshooting/#inspecting-cache-contents","title":"Inspecting cache contents","text":"<p>To inspect which files are currently included in the client cache, run the following command:</p> <pre><code>sudo cvmfs_talk -i software.eessi.io cache list\n</code></pre>"},{"location":"troubleshooting/#checking-cache-consistency","title":"Checking cache consistency","text":"<p>To check the consistency of the CernVM-FS cache, use <code>cvmfs_fsck</code>: <pre><code>sudo time cvmfs_fsck -j 8 /var/lib/cvmfs/shared\n</code></pre></p> <p>This will take a while, depending on the current size of the cache, and how many cores to use are specified (via the <code>-j</code> option).</p>"},{"location":"troubleshooting/#clearing-client-cache","title":"Clearing client cache","text":"<p>To start afresh, you can clear the CernVM-FS client cache: <pre><code>sudo cvmfs_config wipecache\n</code></pre></p>"},{"location":"troubleshooting/#logs","title":"Logs","text":"<p>By default CernVM-FS logs to syslog, which usually corresponds to either <code>/var/log/messages</code> or <code>/var/log/syslog</code>.</p> <p>Scanning these logs for messages produced by <code>cvmfs2</code> may help to determine the root cause of a problem.</p>"},{"location":"troubleshooting/#debug-log","title":"Debug log","text":"<p>For obtaining more detailed information, CernVM-FS provides the <code>CVMFS_DEBUGLOG</code> configuration setting: <pre><code>CVMFS_DEBUGLOG=/tmp/cvmfs-debug.log\n</code></pre></p> <p>CernVM-FS will log more information to the specified debug log file after reloading the CernVM-FS configuration (supported since CernVM-FS 2.11.0).</p> <p>Debug logging is a bit like a firehose - use with care!</p> <p>Note that with debug logging enabled every operation performed by CernVM-FS will be logged, which quickly generates large files and introduces a significant overhead, so it should only be enabled temporarily when trying to obtain more information on a particular problem.</p> <p>Make sure that the debug log file is writable!</p> <p>Make sure that the <code>cvmfs</code> user has write permission to the path specified in <code>CVMFS_DEBUGLOG</code>.</p> <p>If not, you will not only get no debug logging information, but it will also lead to client failures!</p> <p>For more information on debug logging, see the CernVM-FS documentation.</p>"},{"location":"troubleshooting/#logs-via-extended-attributes","title":"Logs via extended attributes","text":"<p>An interesting source of information for mounted CernVM-FS repositories is the extended attributes that CernVM-FS uses, which can accessed via the <code>attr</code> command (see also the CernVM-FS documentation).</p> <p>In particular the <code>logbuffer</code> attribute, which contains the last log messages for that particular repository, which can be accessed without special privileges that are required to access log messages emitted to <code>/var/log/*</code>.</p> <p>For example: <pre><code>$ attr -g logbuffer /cvmfs/software.eessi.io\nAttribute \"logbuffer\" had a 283 byte value for /cvmfs/software.eessi.io:\n[3 Dec 2023 21:01:33 UTC] switching proxy from (none) to http://PROXY_IP:3128 (set proxies)\n[3 Dec 2023 21:01:33 UTC] switching proxy from (none) to http://PROXY_IP:3128 (cloned)\n[3 Dec 2023 21:01:33 UTC] switching proxy from http://PROXY_IP:3128 to DIRECT (set proxies)\n</code></pre></p>"},{"location":"troubleshooting/#other-tools","title":"Other tools","text":""},{"location":"troubleshooting/#general-check","title":"General check","text":"<p>To verify whether the basic setup is sound, run: <pre><code>sudo cvmfs_config chksetup\n</code></pre> which should print \"<code>OK</code>\".</p> <p>If something is wrong, it may report a problem like:</p> <pre><code>Warning: autofs service is not running\n</code></pre> <p>You can also use <code>cvmfs_config</code> to perform a status check, and verify that the command has exit code zero:</p> <pre><code>$ sudo cvmfs_config status\n$ echo $?\n0\n</code></pre>"},{"location":"access/","title":"Accessing CernVM-FS repositories","text":"<ul> <li>Setting up a CernVM-FS client system</li> <li>Alternative ways to access CernVM-FS repositories</li> </ul>"},{"location":"access/alternatives/","title":"Alternative ways to access CernVM-FS repositories","text":"<p>While a native installation of CernVM-FS on the client system is recommended, there are other alternatives available for getting access to CernVM-FS repositories.</p> <p>We briefly cover some of these here, mostly to clarify that there are alternatives available, including some that do not require system administrator permissions.</p>"},{"location":"access/alternatives/#cvmfsexec","title":"<code>cvmfsexec</code>","text":"<p>Using <code>cvmfsexec</code>, mounting of CernVM-FS repositories as an unprivileged user is possible, without having CernVM-FS installed system-wide.</p> <p><code>cvmfsexec</code> supports multiple ways of doing this depending on the OS version and system configuration, more specifically whether or not particular features are enabled, like:</p> <ul> <li>FUSE mounting with <code>fusermount</code>;</li> <li>unprivileged user namespaces;</li> <li>unprivileged namespace fuse mounts;</li> <li>a <code>setuid</code> installation of Singularity 3.4+ (via <code>singcvmfs</code> which uses the <code>--fusemount</code> feature),   or an unprivileged installation of Singularity 3.6+;</li> </ul> <p>Start by cloning the <code>cvmfsexec</code> repository from GitHub, and change to the <code>cvmfsexec</code> directory:</p> <pre><code>git clone https://github.com/cvmfs/cvmfsexec.git\ncd cvmfsexec\n</code></pre> <p>Before using <code>cvmfsexec</code>, you first need to make a <code>dist</code> directory that includes CernVM-FS, configuration files, and scripts. For this, you can run the <code>makedist</code> script that comes with <code>cvmfsexec</code>:</p> <pre><code>./makedist default\n</code></pre> <p>With the <code>dist</code> directory in place, you can use <code>cvmfsexec</code> to run commands in an environment where a CernVM-FS repository is mounted.</p> <p>For example, we can run a script named <code>test_eessi.sh</code> that contains:</p> <pre><code>#!/bin/bash\n\nsource /cvmfs/software.eessi.io/versions/2023.06/init/bash\n\nmodule load TensorFlow/2.13.0-foss-2023a\n\npython -V\npython3 -c 'import tensorflow as tf; print(tf.__version__)'\n</code></pre> <p>which gives: <pre><code>$ ./cvmfsexec software.eessi.io -- ./test_eessi.sh\n\nCernVM-FS: loading Fuse module... done\nCernVM-FS: mounted cvmfs on /home/rocky/cvmfsexec/dist/cvmfs/cvmfs-config.cern.ch\nCernVM-FS: loading Fuse module... done\nCernVM-FS: mounted cvmfs on /home/rocky/cvmfsexec/dist/cvmfs/software.eessi.io\n\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n\nPython 3.11.3\n2.13.0\n</code></pre></p> <p>By default, the CernVM-FS client cache directory will be located in <code>dist/var/lib/cvmfs</code>.</p> <p>For more information on <code>cvmfsexec</code>, see https://github.com/cvmfs/cvmfsexec.</p>"},{"location":"access/alternatives/#apptainer-with-fusemount","title":"Apptainer with <code>--fusemount</code>","text":"<p>If Apptainer is available, you can get access to a CernVM-FS repository by using a container image that includes the CernVM-FS client component (see for example the Docker recipe for the client container used in EESSI, which is available here).</p> <p>Using the <code>--fusemount</code> option you can specify that a CernVM-FS repository should be mounted when starting the container. For example for EESSI, you should use:</p> <pre><code>apptainer ... --fusemount \"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\" ...\n</code></pre> <p>There are a couple of caveats here:</p> <ul> <li> <p>If the configuration for the CernVM-FS repository is provided via the <code>cvmfs-config</code> repository,   you need to instruct Apptainer to also mount that, by using the <code>--fusemount</code> option twice: once for   the <code>cvmfs-config</code> repository, and once for the target repository itself:   <pre><code>FUSEMOUNT_CVMFS_CONFIG=\"container:cvmfs2 cvmfs-config.cern.ch /cvmfs/cvmfs-config.cern.ch\"\nFUSEMOUNT_EESSI=\"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\"\napptainer ... --fusemount \"${FUSEMOUNT_CVMFS_CONFIG}\" --fusemount \"${FUSEMOUNT_EESSI}\" ...\n</code></pre></p> </li> <li> <p>Next to mounting CernVM-FS repositories, you also need to bind mount local writable directories   to <code>/var/run/cvmfs</code>, since CernVM-FS needs write access in those locations (for the CernVM-FS client cache):   <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs}\nexport APPTAINER_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\napptainer ... --fusemount ...\n</code></pre></p> </li> </ul> <p>To try this, you can use the EESSI client container that is available in Docker Hub, to start an interactive shell in which EESSI is available, as follows:</p> <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs}\nexport APPTAINER_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\nFUSEMOUNT_CVMFS_CONFIG=\"container:cvmfs2 cvmfs-config.cern.ch /cvmfs/cvmfs-config.cern.ch\"\nFUSEMOUNT_EESSI=\"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\"\napptainer shell --fusemount \"${FUSEMOUNT_CVMFS_CONFIG}\" --fusemount \"${FUSEMOUNT_EESSI}\" docker://ghcr.io/eessi/client-pilot:centos7\n</code></pre>"},{"location":"access/client/","title":"CernVM-FS client system","text":"<p>The recommended way to gain access to CernVM-FS repositories is to set up a system-wide native installation of CernVM-FS on the client system(s), which comes down to:</p> <ul> <li>Installing the client component of CernVM-FS;</li> <li>Creating a minimal client configuration file (<code>/etc/cvmfs/default.local</code>);</li> <li>Completing the client setup by:<ul> <li>Creating a<code>cvmfs</code> user account and group;</li> <li>Creating the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories;</li> <li>Configuring <code>autofs</code> to enable auto-mounting of repositories (recommended).</li> </ul> </li> </ul> <p>For repositories that are not included in the default CernVM-FS configuration you also need to provide some additional information specific to those repositories in order to access them.</p> <p>This is not a production-ready setup (yet)!</p> <p>While these basic steps are enough to gain access to CernVM-FS repositories, this is not sufficient to obtain a production-ready setup.</p> <p>This is especially true on HPC infrastructure that typically consists of a large number of worker nodes on which software provided by one or more CernVM-FS repositories will be used.</p>"},{"location":"access/client/#installing-cernvm-fs-client","title":"Installing CernVM-FS client","text":"<p>Start with installing the <code>cvmfs</code> package which provides the CernVM-FS client component:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code># install cvmfs-release package to add yum repository\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n# install CernVM-FS client package\nsudo yum install -y cvmfs\n</code></pre> <pre><code># install cvmfs-release package to add apt repository\nsudo apt install lsb-release\ncurl -OL https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nsudo apt update\n\n# install CernVM-FS client package\nsudo apt install -y cvmfs\n</code></pre> <p>If none of the available <code>cvmfs</code> packages are compatible with your system, you can also build CernVM-FS from source.</p>"},{"location":"access/client/#minimal_configuration","title":"Minimal client configuration","text":"<p>Next to installing the CernVM-FS client, you should also create a minimal configuration file for it.</p> <p>This is typically done in <code>/etc/cvmfs/default.local</code>, which should contain something like:</p> <pre><code>CVMFS_CLIENT_PROFILE=\"single\" # a single node setup, not a cluster\nCVMFS_QUOTA_LIMIT=10000\n</code></pre> <p>More information on the structure of <code>/etc/cvmfs</code> and supported configuration settings is available in the CernVM-FS documentation.</p> Client profile setting (click to expand) <p>With <code>CVMFS_CLIENT_PROFILE=\"single\"</code> we specify that this CernVM-FS client should:</p> <ul> <li>Use the proxy server specified via <code>CVMFS_HTTP_PROXY</code>, if that configuration setting is defined;</li> <li>Directly connect to a Stratum-1 replica server that provides the repository being used if no proxy server   is specified via <code>CVMFS_HTTP_PROXY</code>.</li> </ul> <p>As an alternative to defining <code>CVMFS_CLIENT_PROFILE</code>, you can also set <code>CVMFS_HTTP_PROXY</code> to <code>DIRECT</code> to specify that no proxy server should be used by CernVM-FS:</p> <pre><code>CVMFS_HTTP_PROXY=\"DIRECT\"\n</code></pre> Maximum size of client cache (click to expand) <p>The <code>CVMFS_QUOTA_LIMIT</code> configuration setting specifies the maximum size of the CernVM-FS client cache (in MBs).</p> <p>In the example above, we specify that no more than ~10GB should be used for the client cache.</p> <p>When the specified quota limit is reached, CernVM-FS will automatically remove files from the cache according to the Least Recently Used (LRU) policy, until half of the maximum cache size has been freed.</p> <p>The location of the cache directory can be controlled by <code>CVMFS_CACHE_BASE</code> if needed (default: <code>/var/lib/cvmfs</code>), but must be a on a local file system of the client, not a network file system that can be modified by multiple hosts.</p> <p>Using a directory in a RAM disk (like <code>/dev/shm</code>) for the CernVM-FS client cache can be considered if enough memory is available in the client system, which would help reduce latency and start-up performance of software.</p> <p>For more information on cache-related configuration settings, see the CernVM-FS documentation.</p>"},{"location":"access/client/#show-configuration","title":"Show configuration","text":"<p>To show all configuration settings in alphabetical order, including by which configuration file it got set, use <code>cvmfs_config showconfig</code>, for example:</p> <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre> <p>For <code>CVMFS_QUOTA_LIMIT</code>, you should see this in the output:</p> <pre><code>CVMFS_QUOTA_LIMIT=10000    # from /etc/cvmfs/default.local\n</code></pre>"},{"location":"access/client/#completing-the-client-setup","title":"Completing the client setup","text":"<p>To complete the setup of the CernVM-FS client component, we need to make sure that a <code>cvmfs</code> service account and group are present on the system, and the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories exist with the correct ownership and permissions.</p> <p>This should be taken care of by the post-install script that is run when installing the <code>cvmfs</code> package, so you will only need to take action on these aspects if you were installing the CernVM-FS client from source.</p> <p>In addition, it is recommended to update the <code>autofs</code> configuration to enable auto-mounting of CernVM-FS repositories, and to make sure the <code>autofs</code> service is running.</p> <p>All these actions can be performed in one go by running the following command:</p> <pre><code>sudo cvmfs_config setup\n</code></pre> <p>Additional options can be passed to the <code>cvmfs_config setup</code> command to disable some of the actions, like <code>nouser</code> to not create the <code>cvmfs</code> user and group, or <code>noautofs</code> to not update the <code>autofs</code> configuration.</p>"},{"location":"access/client/#autofs","title":"Recommendations for <code>autofs</code>","text":"<p>It is recommended to configure <code>autofs</code> to never unmount repositories due to inactivity, since that can cause problems in specific situations.</p> <p>This can be done by setting additional options in <code>/etc/sysconfig/autofs</code> (on RHEL-based Linux distributions) or <code>/etc/default/autofs</code> (on Debian-based distributions):</p> <pre><code>OPTIONS=\"--timeout 0\"\n</code></pre> <p>The default <code>autofs</code> timeout is typically 5 minutes (300 seconds), which is usually specified in <code>/etc/autofs.conf</code>.</p> Warning when using Slurm's <code>job_container/tmpfs</code> plugin with <code>autofs</code> (click to expand) <p>Slurm versions up to 23.02 had issues when the <code>job_container/tmpfs</code> plugin was being used in combination with <code>autofs</code>. More information can be found at the Slurm bug tracker and the CernVM-FS forum.</p> <p>Slurm version 23.02 includes a fix by providing a <code>Shared</code> option for the <code>job_container/tmpfs</code> plugin, which allows it to work with <code>autofs</code>.</p>"},{"location":"access/client/#using-static-mounts","title":"Using static mounts","text":"<p>If you prefer not to use <code>autofs</code>, you will need to use static mounting, by either:</p> <ul> <li> <p>Manually mounting the CernVM-FS repositories you want to use, for example:   <pre><code>sudo mkdir -p /cvmfs/software.eessi.io\nsudo mount -t cvmfs software.eessi.io /cvmfs/software.eessi.io\n</code></pre></p> </li> <li> <p>Updating <code>/etc/fstab</code> to ensure that the CernVM-FS repositories are mounted at boot time.</p> </li> </ul> <p>Configuring <code>autofs</code> to never unmount due to inactivity is preferable to using static mounts, because the latter requires that every repository is mounted individually, even if is already known in your CernVM-FS configuration. When using <code>autofs</code> you can access all repositories that are known to CernVM-FS through its active configuration.</p> <p>For more information on mounting repositories, see the CernVM-FS documentation.</p>"},{"location":"access/client/#checking-client-setup","title":"Checking client setup","text":"<p>To ensure that the setup of the CernVM-FS client component is valid, you can run:</p> <pre><code>sudo cvmfs_config chksetup\n</code></pre> <p>You should see <code>OK</code> as output of this command.</p>"},{"location":"access/client/#default-repositories","title":"Default repositories","text":"<p>The default configuration of CernVM-FS, provided by the <code>cvmfs-config-default</code> package, provides the public keys and configuration for a number of commonly used CernVM-FS repositories.</p> <p>One particular repository included in the default CernVM-FS configuration is <code>cvmfs-config.cern.ch</code>, which is a CernVM-FS config repository that provides public keys and configuration for additional flagship CernVM-FS repositories, like <code>software.eessi.io</code>:</p> <pre><code>$ ls /cvmfs/cvmfs-config.cern.ch/etc/cvmfs\ncommon.conf  config.d  default.conf  domain.d  keys\n\n$ find /cvmfs/cvmfs-config.cern.ch/etc/cvmfs -type f -name '*eessi*'\n/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/domain.d/eessi.io.conf\n/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/keys/eessi.io/eessi.io.pub\n</code></pre> <p>That means we now already have access to the EESSI CernVM-FS repository:</p> <pre><code>$ ls /cvmfs/software.eessi.io\nREADME.eessi  host_injections  versions\n</code></pre>"},{"location":"access/client/#inspecting_configuration","title":"Inspecting repository configuration","text":"<p>To check whether a specific CernVM-FS repository is accessible, we can probe it:</p> <pre><code>$ cvmfs_config probe software.eessi.io\nProbing /cvmfs/software.eessi.io... OK\n</code></pre> <p>To view the configuration for a specific repository, use <code>cvmfs_config showconfig</code>: <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre></p> <p>To check the active configuration for a specific repository used by the running CernVM-FS instance, use <code>cvmfs_talk -i &lt;repo&gt; parameters</code> (which requires admin privileges):</p> <pre><code>sudo cvmfs_talk -i software.eessi.io parameters\n</code></pre> <p><code>cvmfs_talk</code> requires that the repository is currently mounted. If not, you will see an error like this:</p> <pre><code>$ sudo cvmfs_talk -i software.eessi.io parameters\nSeems like CernVM-FS is not running in /var/lib/cvmfs/shared (not found: /var/lib/cvmfs/shared/cvmfs_io.software.eessi.io)\n</code></pre>"},{"location":"access/client/#accessing-a-repository","title":"Accessing a repository","text":"<p>To access the contents of the repository, just use the corresponding subdirectory as if it were a local filesystem.</p> <p>While the contents of the files you are accessing are not actually available on the client system the first time they are being accessed, CernVM-FS will automatically downloaded them in the background, providing the illusion that the whole repository is already there.</p> <p>We like to refer to this as \"streaming\" of software installations, much like streaming music or video services.</p> <p>To start using EESSI just source the initialisation script included in the repository:</p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash\n</code></pre> <p>You may notice some \"lag\" when files are being accessed, or not, depending on the network latency.</p>"},{"location":"access/client/#additional-repositories","title":"Additional repositories","text":"<p>To access additional CernVM-FS repositories beyond those that are available by default, you will need to:</p> <ul> <li>Add the public keys for those repositories into a domain-specific subdirectory of <code>/etc/cvmfs/keys/</code>;</li> <li>Add the configuration for those repositories into <code>/etc/cvmfs/domain.d</code> (domain-specific) or <code>/etc/cvmfs/config.d</code> (repository-specific).</li> </ul> <p>Examples are available in the <code>etc/cvmfs</code> subdirectory of the config-repo GitHub repository.</p>"},{"location":"appendix/terminology/","title":"Terminology","text":"<p>An overview of terms used in the context of EESSI, in alphabetical order.</p>"},{"location":"appendix/terminology/#cvmfs","title":"CernVM-FS","text":"<p>(see What is CernVM-FS?)</p>"},{"location":"appendix/terminology/#client","title":"Client","text":"<p>A client in the context of CernVM-FS is a computer system on which a CernVM-FS repository is being accessed, on which it will be presented as a POSIX read-only file system in a subdirectory of <code>/cvmfs</code>.</p>"},{"location":"appendix/terminology/#proxy","title":"Proxy","text":"<p>A proxy, also referred to as squid proxy, is a forward caching proxy server which acts as an intermediary between a CernVM-FS client and the Stratum-1 replica servers.</p> <p>It is used to improve the latency observed when accessing the contents of a repository, and to reduce the load on the Stratum-1 replica servers.</p> <p>A commonly used proxy is Squid.</p> <p>For more information on proxies, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#repository","title":"Repository","text":"<p>A CernVM-FS repository is where the files and directories that you want to distribute via CernVM-FS are stored, which usually correspond to a collection of software installations.</p> <p>It is a form of content-addressable storage (CAS), and is the single source of (new) data for the file system being presented as a subdirectory of <code>/cvmfs</code> on client systems that mount the repository.</p> <p>Note</p> <p>A CernVM-FS repository includes software installations, not software packages like RPMs.</p>"},{"location":"appendix/terminology/#software-installations","title":"Software installations","text":"<p>An important distinction for a CernVM-FS repository compared to the more traditional notion of a software repository is that a CernVM-FS repository provides access to the individual files that collectively form a particular software installation, as opposed to housing a set of software packages like RPMs, each of which being a collection of files for a particular software installation that are packed together in a single package to distribute as a whole.</p> <p>Note</p> <p>This is an important distinction, since CernVM-FS enables only downloading the specific files that are required to perform a particular task with a software installation, which often is a small subset of all files that are part of that software installation.</p>"},{"location":"appendix/terminology/#stratum1","title":"Stratum 1 replica server","text":"<p>A Stratum 1 replica server, often simply referred to a Stratum 1 (Stratum One), is a standard web server that acts as a mirror server for one or more CernVM-FS repositories.</p> <p>It holds a complete copy of the data for each CernVM-FS repository it serves, and automatically synchronises with the Stratum 0.</p> <p>There is typically a network of several Stratum 1 servers for a CernVM-FS repository, which are geographically distributed.</p> <p>Clients can be configured to automatically connect to the closest Stratum 1 server by using the CernVM-FS GeoAPI.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"eessi/","title":"EESSI","text":""},{"location":"eessi/#european-environment-for-scientific-software-installations","title":"European Environment for Scientific Software Installations","text":"<ul> <li>What is EESSI?</li> <li>Motivation &amp; Goals</li> <li>Inspiration</li> <li>High-level design</li> <li>Using EESSI</li> <li>Getting support</li> </ul>"},{"location":"eessi/high-level-design/","title":"High-level design of EESSI","text":"<p>The design of EESSI is very similar to that of the Compute Canada software stack it is inspired by, and is aligned with the motivation and goals of the project.</p> <p>In the remainder of this section of the tutorial, we will explore the layered structure of the EESSI software stack, and how to use it.</p> <p>In the next section will cover in detail how you can get access to EESSI.</p>"},{"location":"eessi/high-level-design/#layered-structure","title":"Layered structure","text":"<p>To provide optimized installations of scientific software stacks for a diverse set of system architectures, the EESSI project consists of 3 layers, which are constructed by leveraging various open source software projects:</p> <ul> <li>the filesystem layer to distribute the software stack;</li> <li>the compatibility layer to level the ground across different client operating systems;</li> <li>the software layer to run optimized applications and provided their dependencies</li> </ul> <p> </p>"},{"location":"eessi/high-level-design/#filesystem_layer","title":"Filesystem layer","text":"<p>The filesystem layer uses  CernVM-FS**](https://multixscale.github.io/cvmfs-tutorial-hpc-best-practices/cvmfs/what-is-cvmfs/) to distribute the EESSI software stack to client systems.</p> <p>As presented in the previous section, CernVM-FS is a mature open source software project that was created exactly for this purpose: to distribute software installations worldwide reliably and efficiently in a scalable way. As such, it aligns very well with the goals of EESSI.</p> <p>The CernVM-FS repository for EESSI is <code>/cvmfs/software.eessi.io</code>, which is part of the default CernVM-FS configuration since 21 November 2023.</p> <p>To gain access to it, no other action is required then installing (and configuring) the client component of CernVM-FS.</p> Note on the EESSI pilot repository (click to expand) <p>There is also a \"pilot\" CernVM-FS repository for EESSI (<code>/cvmfs/pilot.eessi-hpc.org</code>), which was primarily used to gain experience with CernVM-FS in the early years of the EESSI project.</p> <p>Although it is still available currently, we do not recommend using it.</p> <p>Not only will you need to install the CernVM-FS configuration for EESSI to gain access to it, there also are no guarantees that the EESSI pilot repository will remain stable or even available, nor that the software installations it provides are actually functional, since it may be used for experimentation purposes by the EESSI maintainers.</p>"},{"location":"eessi/high-level-design/#compatibility_layer","title":"Compatibility layer","text":"<p>The compatibility layer of EESSI levels the ground across different (versions of) the Linux operating system (OS) of client systems that use the software installations provided by EESSI.</p> <p>It consists of a limited set of libraries and tools that are installed in a non-standard filesystem location (a \"prefix\"), which were built from source for the supported CPU families using Gentoo Prefix.</p> <p>The installation path of the EESSI compatibility layer corresponds to the <code>compat</code> subdirectory of a specific version of EESSI (like <code>2023.06</code>) in the EESSI CernVM-FS repository, which is specific to a particular type of OS (currently only <code>linux</code>) and CPU family (currently <code>x86_64</code> and <code>aarch64</code>):</p> <pre><code>$ ls /cvmfs/software.eessi.io/versions/2023.06/compat\nlinux\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux\naarch64  x86_64\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64\nbin  etc  lib  lib64  opt  reprod  run  sbin  stage1.log  stage2.log  stage3.log  startprefix  tmp  usr  var\n\n$ ls -l /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64\ntotal 4923\n-rwxr-xr-x 1 cvmfs cvmfs  210528 Nov 15 11:22 ld-linux-x86-64.so.2\n...\n-rwxr-xr-x 1 cvmfs cvmfs 1876824 Nov 15 11:22 libc.so.6\n...\n-rwxr-xr-x 1 cvmfs cvmfs  911600 Nov 15 11:22 libm.so.6\n...\n</code></pre> <p>Libraries included in the compatibility layer can be used on any Linux client system, as long as the CPU family is compatible and taken into account.</p> <pre><code>$ uname -m\nx86_64\n\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux release 8.8 (Ootpa)\n\n$ /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64/libc.so.6\nGNU C Library (Gentoo 2.37-r7 (patchset 10)) stable release version 2.37.\n...\n</code></pre> <p>By making sure that the software installations included in EESSI only rely on tools and libraries provided by the compatibility layer, and do not (directly) require anything from the client OS, we can ensure that they can be used in a broad variety of Linux systems, regardless of the (version of) Linux distribution being used.</p> <p>Note</p> <p>This is very similar to the OS tools and libraries that are included in container images, except that no container runtime is involved here.</p> <p>Typically only CernVM-FS is used to provide the entire software (stack).</p>"},{"location":"eessi/high-level-design/#software_layer","title":"Software layer","text":"<p>The top layer of EESSI is called the software layer, which contains the actual scientific software applications and their dependencies.</p>"},{"location":"eessi/high-level-design/#easybuild","title":"EasyBuild to install software","text":"<p>Building, managing, and optimising the software installations included in the software layer is layer is done using EasyBuild, a well-established software build and installation framework for managing (scientific) software stacks on High-Performance Computing (HPC) systems.</p>"},{"location":"eessi/high-level-design/#lmod","title":"Lmod as user interface","text":"<p>Next to installing the software itself, EasyBuild also automatically generates environment module files. These files, which are essentially small Lua scripts, are consumed via Lmod, a modern implementation of the concept of environment modules which provides a user-friendly interface to end users of EESSI.</p>"},{"location":"eessi/high-level-design/#cpu_detection","title":"CPU detection via <code>archspec</code> or <code>archdetect</code>","text":"<p>The initialisation script that is included in the EESSI repository automatically detects the CPU family and microarchitecture of a client system by leveraging either <code>archspec</code>, a small Python library, or <code>archdetect</code>, a minimal pure bash implementation of the same concept.</p> <p>Based on the features of the detected CPU microarchitecture, the EESSI initialisation script will automatically select the best suited subdirectory of the software layer that contains software installations that are optimised for that particular type of CPU, and update the session environment to start using it.</p>"},{"location":"eessi/high-level-design/#software_layer_structure","title":"Structure of the software layer","text":"<p>For now, we just briefly show the structure of <code>software</code> subdirectory that contains the software layer of a particular version of EESSI below.</p> <p>The <code>software</code> subdirectory is located at the same level as the <code>compat</code> directory for a particular version of EESSI, along with the <code>init</code> subdirectory that provides initialisation scripts:</p> <pre><code>$ cd /cvmfs/software.eessi.io/versions/2023.06\n$ ls\ncompat  init  software\n</code></pre> <p>In the <code>software</code> subdirectory, a subtree of directories is located that contains software installations that are specific to a particular OS family (only <code>linux</code> currently) and a specific CPU microarchitecture (with <code>generic</code> as a fallback):</p> <pre><code>$ ls software\nlinux\n\n$ ls software/linux\naarch64  x86_64\n\n$ ls software/linux/aarch64\ngeneric  neoverse_n1  neoverse_v1\n\n$ ls software/linux/x86_64\namd  generic  intel\n\n$ ls software/linux/x86_64/amd\nzen2  zen3\n\n$ ls software/linux/x86_64/intel\nhaswell  skylake_avx512\n</code></pre> <p>Each subdirectory that is specific to a particular CPU microarchitecure provides the actual optimised software installations (in <code>software</code>) and environment module files (in <code>modules/all</code>).</p> <p>Here we explore the path that is specific to AMD Milan CPUs, which have the Zen3 microarchitecture, focusing on the installations of OpenBLAS:</p> <pre><code>$ ls software/linux/x86_64/amd/zen3\nmodules  software\n\n$ ls software/linux/x86_64/amd/zen3/software\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/\n0.3.21-GCC-12.2.0  0.3.23-GCC-12.3.0\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/\nbin  easybuild  include  lib  lib64\n\n$ ls software/linux/x86_64/amd/zen3/modules/all\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/modules/all/OpenBLAS\n0.3.21-GCC-12.2.0.lua  0.3.23-GCC-12.3.0.lua\n</code></pre> <p>Each of the other subdirectories for specific CPU microarchitectures will have the exact same structure, and provide the same software installations and accompanying environment module files to access them with Lmod.</p> <p>A key aspect here is that binaries and libraries that make part of the software installations included in the EESSI software layer only rely on libraries provided by the compatibility layer and/or other software installations in the EESSI software layer.</p> <p>See for example libraries to which the OpenBLAS library links:</p> <pre><code>$ ldd software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/lib/libopenblas.so\n    linux-vdso.so.1 (0x00007ffd4373d000)\n    libm.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libm.so.6 (0x000014d0884c8000)\n    libgfortran.so.5 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgfortran.so.5 (0x000014d087115000)\n    libgomp.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgomp.so.1 (0x000014d088480000)\n    libc.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libc.so.6 (0x000014d086f43000)\n    /lib64/ld-linux-x86-64.so.2 (0x000014d08837e000)\n    libpthread.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libpthread.so.0 (0x000014d088479000)\n    libdl.so.2 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libdl.so.2 (0x000014d088474000)\n    libquadmath.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libquadmath.so.0 (0x000014d08842d000)\n    libgcc_s.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgcc_s.so.1 (0x000014d08840d000)\n</code></pre> Note on <code>/lib64/ld-linux-x86-64.so.2</code> (click to expand) <p>The <code>/lib64/ld-linux-x86-64.so.2</code> path, which corresponds to the dynamic linker/loader of the Linux client OS, that is shown in the output of <code>ldd</code> above is a bit misleading.</p> <p>It only pops up because we are running the <code>ldd</code> command provided by the client OS, which typically resides at <code>/usr/bin/ldd</code>.</p> <p>When actually running software provided by the EESSI software layer, the loader provided by the EESSI compatibility layer is used to launch binaries.</p> <p>We will explore the EESSI software layer a bit more when we demonstrate how to use the software installations provided the EESSI CernVM-FS repository.</p> <p>(next: Using EESSI)</p>"},{"location":"eessi/inspiration/","title":"Inspiration for EESSI","text":"<p>The EESSI concept is heavily inspired by software stack provided by the Digital Research Alliance of Canada (a.k.a. The Alliance, formerly known as Compute Canada), which is a shared software stack used on all national host sites for Advanced Research Computing in Canada that is distributed across Canada (and beyond) using CernVM-FS.</p> <p>EESSI is significantly more ambitious in its goals however, in various ways.</p> <p>It intends to support a broader range of system architectures than what is currently supported by the Compute Canada software stack, like Arm 64-bit microprocessors, accelerators beyond NVIDIA GPUs, etc.</p> <p>In addition, EESSI is set up to be a community project, by setting up services and infrastructure to automate the software build and installation process as much as possible, providing extensive documentation and support to end users, user support teams, and system administrators who want to employ EESSI, and allowing contributors to propose additions to the software stack.</p> <p>The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\".</p> <p>It has also been presented at the 5th EasyBuild User Meeting, see slides and talk recording.</p> <p>More information on the Compute Canada software stack is available in their documentation, and in their overview of available software.</p> <p>(next: High-level Overview of EESSI)</p>"},{"location":"eessi/motivation-goals/","title":"Motivation &amp; Goals of EESSI","text":""},{"location":"eessi/motivation-goals/#motivation","title":"Motivation","text":"<p>EESSI is motivated by the observation that the landscape of computational science is changing in various ways, including:</p> <ul> <li>Increasing diversity in system architectures: additional families of general-purpose   microprocessors including Arm 64-bit (<code>aarch64</code>) and   RISC-V on top of the well-established Intel and AMD processors (both <code>x86_64</code>),   and different types of GPUS (NVIDIA, AMD, Intel);</li> <li>Rapid expansion of computational science beyond traditional domains like physics and computational chemistry,   including bioinformatis, Machine Learning (ML) and Artificial Intelligence (AI), etc.,   which leads to a significant growth of the software stack that is used for running scientific workloads;</li> <li>Emergence of commercial cloud infrastructure (Amazon EC2,   Microsoft Azure, ...)   that has competitive advantages over on-premise infrastructure for computational workloads, such as near-instant   availability, increased flexibility, a broader variety of hardware platforms, and faster access to   new generations of microprocessors;</li> <li>Limited manpower that is available in the HPC user support teams that are responsible for helping   scientists with running the software they require on high-end (and complex) infrastructure like supercomputers   (and beyond);</li> </ul> <p>Collectively, these indicate that there is a strong need for more collaboration on building and installing scientific software to avoid duplicate work across computational scientists and HPC user support teams.</p>"},{"location":"eessi/motivation-goals/#goals","title":"Goals","text":"<p>The main goal of EESSI is to provide a collection of scientific software installations that work across a wide range of different platforms, including HPC clusters, cloud infrastructure, and personal workstations and laptops, without making compromises on the performance of that software.</p> <p>While initially the focus of EESSI is to support Linux systems with established system architectures like AMD + Intel CPUs and NVIDIA GPUs, the ambition is to also cover emerging technologies like Arm 64-bit CPUs, other accelerators like the AMD Instinct and Intel Xe, and eventually also RISC-V microprocessors.</p> <p>The software installations included in EESSI are optimized for specific generations of microprocessors by targeting a variety of instruction set architectures (ISAs), like for example Intel and AMD processors supporting the AVX2 or AVX-512 instructions, and Arm processors that support SVE instructions.</p> <p>(next: Inspiration for EESSI)</p>"},{"location":"eessi/support/","title":"Getting support for EESSI","text":"<p>Thanks to the funding provided by the MultiXscale EuroHPC JU Centre-of-Excellence, a dedicated support team is available to provide help on accessing or using EESSI.</p> <p>If you have any questions, or if you are experiencing problems, do not hesitate to reach out by either opening an issue in the EESSI support portal, or sending an email to <code>support@eessi.io</code>.</p> <p>For more information, see the support section of the EESSI documentation.</p> <p>(next: CernVM-FS client system)</p>"},{"location":"eessi/using-eessi/","title":"Using EESSI","text":"<p>Using the software installations provided by the EESSI CernVM-FS repository <code>software.eessi.io</code> is fairly straightforward.</p> <p>Let's break it down step by step.</p>"},{"location":"eessi/using-eessi/#0-is-eessi-available","title":"0) Is EESSI available?","text":"<p>First, check whether the EESSI  CernVM-FS repository is available on your system.</p> <p>Try checking the contents of the <code>/cvmfs/software.eessi.io</code> directory with the <code>ls</code> command:</p> <pre><code>$ ls /cvmfs/software.eessi.io\nREADME.eessi  host_injections  versions\n</code></pre> <p>If you see an error message like \"<code>No such file or directory</code>\", then either the CernVM-FS client is not installed on your system, or the configuration for the EESSI repository is not available. In that case, you may want to revisit the Accessing a CernVM-FS repository section, or go through the Troubleshooting section.</p> Don't be fooled by <code>autofs</code> (click to expand) <p>The <code>/cvmfs</code> directory may seem empty at first, because CernVM-FS repositories are automatically mounted as they are accessed via <code>autofs</code>.</p> <p>So rather than just using \"<code>ls /cvmfs/</code>\" to check which CernVM-FS repositories are available on your system, you should try to directly access a specific repository as shown above for EESSI with <code>ls /cvmfs/software.eessi.io</code> .</p> <p>For more information on various aspects of mounting of CernVM-FS repositories, see the CernVM-FS documentation.</p>"},{"location":"eessi/using-eessi/#init","title":"1) Initialise shell environment","text":"<p>If the EESSI repository is available, you can proceed to preparing your shell environment for using a particular version of EESSI by sourcing the provided initialisation script by running the <code>source</code> command:</p> <pre><code>$ source /cvmfs/software.eessi.io/versions/2023.06/init/bash\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n</code></pre> Details on changes made to the shell environment (click to expand) <p>The initialisation script is a simple bash script that changes a couple of environment variables:</p> <ul> <li>A set of <code>$EESSI_*</code> environment variables is defined;</li> <li>The <code>$PS1</code> environment variable that specifies the shell prompt   is updated to indicate that your shell session has been initialised for EESSI;</li> <li>The location of the tools provided by the EESSI compatibility layer are prepended to the <code>$PATH</code> environment variable;</li> <li>Lmod, which is included in the EESSI compatibility layer, is initialised to ensure that the <code>module</code> command is defined,   and that the Lmod spider cache that is included in the EESSI software layer is picked up;</li> <li>The location to the software installations that are optimised for the CPU microarchitecture of the client system   is prepended to the <code>$MODULEPATH</code> environment variable by running a \"<code>module use</code>\" command.</li> </ul> <p>Note how the CPU microarchitecture is being auto-detected, which determines which path that points to a set of environment module files is used to update <code>$MODULEPATH</code>.</p> <p>This ensures that the modules that will be loaded provide access to software installations from the EESSI software layer that are optimised for the system you are using EESSI on.</p>"},{"location":"eessi/using-eessi/#2-load-modules","title":"2) Load module(s)","text":"<p>After initialising your shell environment for using EESSI, you can start exploring the EESSI software layer using the <code>module</code> command.</p> <p>Using <code>module avail</code> (or <code>ml av</code>), you can check which software is available. Without extra arguments, <code>module avail</code> will produce an overview of all available software. By passing an extra argument you can filter the results and search for specific software:</p> <pre><code>$ module avail tensorflow\n\n----- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all -----\n\n    TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>To start using software you should load the corresponding environment module files using <code>module load</code> (or <code>ml</code>). For example:</p> <pre><code>$ module load TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>A <code>module load</code> command usually does not produce any output, but it updates your shell environment to make the software ready to use.</p> <p>For more information on the <code>module</code> command, see the User Guide for Lmod.</p>"},{"location":"eessi/using-eessi/#3-use-software","title":"3) Use software","text":"<p>After loading a module, you should be able to use the corresponding software.</p> <p>For example, after loading the <code>TensorFlow/2.13.0-foss-2023a</code> module, you can start a Python session and play with the <code>tensorflow</code> Python package:</p> <pre><code>$ python\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.13.0'\n</code></pre> <p>Keep in mind that you are using a Python installation provided by the EESSI software layer here, not the Python version that may be provided by your client OS:</p> <pre><code>$ command -v python\n/cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/software/Python/3.11.3-GCCcore-12.3.0/bin/python\n</code></pre> Initial start-up delay (click to expand) <p>You may notice a bit of \"lag\" initially when starting to use software provided by the EESSI software layer.</p> <p>This is expected, since CernVM-FS may need to first download the files that are required to run the software you are using.</p> <p>You should not observe any significant start-up delays anymore when running the same software shortly after, since then CernVM-FS will be able to serve the necessary files from the local client cache.</p> <p>(next: Getting support for EESSI)</p>"},{"location":"eessi/what-is-eessi/","title":"What is EESSI?","text":"<p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in the HPC (High Performance Computing) community.</p> <p>EESSI provides a common stack of optimized scientific software installations that work on any Linux distribution, and currently supports both <code>x86_64</code> (AMD/Intel) and <code>aarch64</code> (Arm 64-bit) systems, which is distributed via CernVM-FS.</p> <p>(next: Motivation &amp; Goals of EESSI)</p>"}]}